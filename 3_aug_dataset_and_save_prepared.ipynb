{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. import dsml base module\n",
    "from dsml_s8e.module import DSMLModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#2. specify parameters\n",
    "\n",
    "# Parameters\n",
    "run_parameters = {\n",
    "    \"env_name\": \"user\",\n",
    "    \"product_name\": \"cv_example\",\n",
    "    \"stand_name\": \"YOLOX_mmdet\",\n",
    "    \"docker_image\": \"cv-no-gpu:latest\",\n",
    "    \"conda_env\": \"gpu\",\n",
    "    \"business_report_repo\": \"\",\n",
    "    \"infra\": {},\n",
    "    \"comment\": {},\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    \"loggingLevel\"       : \"INFO\",\n",
    "    \"FILTER_EMPTY_GT\"    : False,\n",
    "    \"AUGMENTATION_TYPE\"  : 1,\n",
    "    \"MIN_OBJECT_SIZE\"    : 5,\n",
    "    \"MAX_SIZE\"           : 1024,\n",
    "    \"KEEP_RATIO\"         : True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(parameters, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dsml_s8e.spark import SparkEnvironment\n",
    "\n",
    "SparkEnvironment.stopSparkSession()\n",
    "spark = SparkEnvironment.runSparkSession(0)\n",
    "SparkEnvironment.showSparkUI()\n",
    "\n",
    "import atexit\n",
    "_=atexit.register(SparkEnvironment.stopSparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "module = DSMLModule(parameters, run_parameters)\n",
    "\n",
    "cache_urls = module.make_cache_urls(\n",
    "    new_cache_entity_names=[\"aug_dataset\"],\n",
    "    last_cache_entity_names=[\"cache_data\"]\n",
    ")\n",
    "\n",
    "resource_urls = module.make_component_resource_urls(\n",
    "    \"1_data_import\", \n",
    "    entity_names=[\"split_config\"]\n",
    ")\n",
    "\n",
    "a7s_urls = module.make_artifacts_urls(\n",
    "    entity_names=[\n",
    "        \"train_coco_data\",\n",
    "        \"eval_coco_data\",\n",
    "        \"cache_config\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "module.print_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dsml_s8e.store import DSMLStore\n",
    "\n",
    "DSMLStore.copy_file_to_cache(\n",
    "    os.path.join(resource_urls.split_config, 'split_config.json'),\n",
    "    os.path.join(cache_urls.cache_data, 'split_config.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "split_config_path = os.path.join(cache_urls.cache_data, 'split_config.json')\n",
    "\n",
    "with open(split_config_path) as f:\n",
    "    SPLIT_CONFIG = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    TRAIN_DATASET_DIRS=[os.path.join(cache_urls.cache_data, d) for d in SPLIT_CONFIG['train_datasets']],\n",
    "    EVAL_DATASET_DIRS=[os.path.join(cache_urls.cache_data, d) for d in SPLIT_CONFIG['eval_datasets']],\n",
    ")\n",
    "\n",
    "cache_config_path = os.path.join(cache_urls.cache_data, 'config.json')\n",
    "\n",
    "with open(cache_config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "    \n",
    "!cat {cache_config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_size           = parameters['MAX_SIZE']\n",
    "filter_empty_gt    = parameters['FILTER_EMPTY_GT']\n",
    "eval_file_folders  = CONFIG.get('EVAL_DATASET_DIRS')\n",
    "train_file_folders = CONFIG.get('TRAIN_DATASET_DIRS')\n",
    "min_gt_bbox_wh     = parameters.get('MIN_OBJECT_SIZE'), parameters.get('MIN_OBJECT_SIZE')\n",
    "\n",
    "CONFIG = dict(CONFIG, **parameters)\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.coco import join_coco_files, load as load_coco\n",
    "from utils.coco import show_item\n",
    "from utils.coco import get_dataset\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(cache_urls.aug_dataset, exist_ok=True)\n",
    "\n",
    "eval_max_object_size = 0\n",
    "if eval_file_folders:\n",
    "    eval_coco_file = os.path.join(cache_urls.aug_dataset, 'eval.json')\n",
    "    eval_max_object_size = join_coco_files(eval_file_folders, output_file=eval_coco_file)\n",
    "\n",
    "print()\n",
    "\n",
    "train_max_object_size = 0\n",
    "if train_file_folders:\n",
    "    train_coco_file = os.path.join(cache_urls.aug_dataset, 'train.json')\n",
    "    train_max_object_size = join_coco_files(train_file_folders, output_file=train_coco_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overlap = 0.5\n",
    "\n",
    "print(f\"{overlap=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_pipeline      = [dict(type=\"LoadImageFromFile\")]\n",
    "post_loader_pipeline = [dict(type=\"LoadImageFromFile\")]\n",
    "\n",
    "collect_pipeline = [\n",
    "    dict(type=\"Collect\", keys=[\"img\", \"gt_bboxes\", \"gt_labels\", \"gt_masks\"]\n",
    "         , meta_keys=('filename', 'img_shape')),\n",
    "]\n",
    "\n",
    "resize_pipeline = [\n",
    "    dict(type='Resize', img_scale=(CONFIG['MAX_SIZE'], CONFIG['MAX_SIZE']), keep_ratio=CONFIG['KEEP_RATIO']),\n",
    "    dict(type='FilterAnnotationsBugFix', min_gt_bbox_wh=(5,5)),\n",
    "    \n",
    "]\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type=\"LoadAnnotations\", with_bbox=True, with_mask=True, poly2mask=False),\n",
    "    dict(type='FilterAnnotationsBugFix', min_gt_bbox_wh=min_gt_bbox_wh),\n",
    "]\n",
    "\n",
    "eval_pipeline = [\n",
    "    dict(type=\"LoadAnnotations\", with_bbox=True, with_mask=True, poly2mask=False),\n",
    "    dict(type='FilterAnnotationsBugFix', min_gt_bbox_wh=min_gt_bbox_wh),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mmdet_utils.pipelines.train_pipelines import *\n",
    "from mmdet_utils.pipelines.test_pipelines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mmdet\n",
    "import mmcv\n",
    "import torch\n",
    "\n",
    "print(mmdet.__version__)\n",
    "print(mmcv.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mmdet_fp = \"https://files.pythonhosted.org/packages/33/da/c979cca457c732e598131a3939c15d7d57b14b7ba6b79cba52a0cd604d99/mmdet-2.25.1-py3-none-any.whl\"\n",
    "# mmcv_fp  = \"https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/mmcv_full-1.6.0-cp39-cp39-manylinux1_x86_64.whl\"\n",
    "\n",
    "# assert mmdet.__version__ == '2.25.1', f'{mmdet.__version__} is not (2.25.1) download from \"{mmdet_fp}\"'\n",
    "# assert mmcv.__version__ == '1.6.0', f'{mmcv.__version__} is not (1.6.0) download from \"{mmcv_fp}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_datasets = []\n",
    "if eval_max_object_size > 0:        \n",
    "    if CONFIG['AUGMENTATION_TYPE'] in [0]:\n",
    "        eval_datasets.append({\n",
    "            \"aug_type\": 0,\n",
    "            \"dataset\" : get_dataset(eval_coco_file, \n",
    "                                    loader_pipeline + eval_pipeline + collect_pipeline, filter_empty_gt),\n",
    "        })\n",
    "\n",
    "    if CONFIG['AUGMENTATION_TYPE'] in [1]:\n",
    "        eval_datasets.append({\n",
    "            \"aug_type\": 1,\n",
    "            \"dataset\" : get_dataset(eval_coco_file, \n",
    "                                    loader_pipeline + eval_pipeline + resize_pipeline + collect_pipeline, filter_empty_gt)\n",
    "        })\n",
    "\n",
    "    for eval_dataset in eval_datasets:\n",
    "        print(eval_dataset['dataset'])\n",
    "        \n",
    "        show_item(eval_dataset['dataset'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "\n",
    "if train_max_object_size > 0:\n",
    "    if CONFIG['AUGMENTATION_TYPE'] in [0]:\n",
    "        train_datasets.append({\n",
    "            \"aug_type\": 0,\n",
    "            \"dataset\" : get_dataset(train_coco_file, \n",
    "                                    loader_pipeline + train_pipeline + collect_pipeline, filter_empty_gt),\n",
    "        })\n",
    "\n",
    "    if CONFIG['AUGMENTATION_TYPE'] in [1]:\n",
    "        train_datasets.append({\n",
    "            \"aug_type\": 1,\n",
    "            \"dataset\" : get_dataset(train_coco_file, \n",
    "                                    loader_pipeline + train_pipeline + resize_pipeline + collect_pipeline, filter_empty_gt)\n",
    "        })\n",
    "\n",
    "    for train_dataset in train_datasets:\n",
    "        print(train_dataset['dataset'])\n",
    "        \n",
    "        show_item(train_dataset['dataset'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.coco.item import item_to_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset(dataset, out_folder, max_size, save_all_image):\n",
    "    pbar = tqdm(total=len(dataset))\n",
    "    pbar.set_description(f\"Processing [{dataset.ann_file}]\")\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(8) as executor:\n",
    "        futures = []\n",
    "        for i in range(len(dataset)):\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    item_to_coco, dataset, i, out_folder, max_size, dataset.cat_ids, dataset.CLASSES, overlap=overlap, save_all_image=save_all_image\n",
    "                )\n",
    "            )\n",
    "            pbar.update(0.5)\n",
    "        \n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            _result = future.result()\n",
    "            if type(_result) is Exception:\n",
    "                try:\n",
    "                    executor.shutdown(wait=False)\n",
    "                except OSError:\n",
    "                    pass\n",
    "                \n",
    "                break\n",
    "            \n",
    "            pbar.update(0.5)\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eval_out_folder = os.path.join(cache_urls.aug_dataset, 'eval_data')\n",
    "!rm -rf {eval_out_folder}/*\n",
    "\n",
    "for eval_dataset in eval_datasets:\n",
    "    save_all_image = eval_dataset['aug_type'] == 1\n",
    "    process_dataset(eval_dataset['dataset'], eval_out_folder, max_size, save_all_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_output_file = os.path.join(eval_out_folder, 'all_coco.json')\n",
    "join_coco_files(eval_out_folder, output_file=eval_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preview_pipeline = [pp for pp in eval_pipeline if \"Filter\" not in pp['type']]\n",
    "\n",
    "eval_dataset = get_dataset(eval_output_file, post_loader_pipeline + preview_pipeline + collect_pipeline)\n",
    "if len(eval_dataset):\n",
    "    show_item(eval_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_out_folder = osp.join(cache_urls.aug_dataset, 'train_data')\n",
    "!rm -rf {train_out_folder}/*\n",
    "\n",
    "for train_dataset in train_datasets:\n",
    "    save_all_image = train_dataset['aug_type'] == 1\n",
    "    process_dataset(train_dataset['dataset'], train_out_folder, max_size, save_all_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_output_file = os.path.join(train_out_folder, 'all_coco.json')\n",
    "join_coco_files(train_out_folder, output_file=train_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preview_pipeline = [pp for pp in train_pipeline if \"Filter\" not in pp['type']]\n",
    "\n",
    "train_dataset = get_dataset(train_output_file, post_loader_pipeline + preview_pipeline + collect_pipeline)\n",
    "if len(train_dataset):\n",
    "    show_item(train_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for file in [eval_output_file, train_output_file]:\n",
    "    all_coco_data = load_coco(file)\n",
    "\n",
    "    mean = []\n",
    "    std = []\n",
    "\n",
    "    for image in all_coco_data['images']:\n",
    "        if len(image.get('mean', [])) == 3:\n",
    "            mean.append(image['mean'])\n",
    "            std.append(image['std'])\n",
    "\n",
    "    mean = np.array(mean).mean(axis=0).round(2).tolist()\n",
    "    std = np.array(std).mean(axis=0).round(2).tolist()\n",
    "\n",
    "    print(f\"{file = }\")\n",
    "    print(f\"{mean = }\")\n",
    "    print(f\"{std  = }\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {cache_urls.aug_dataset}/eval_data | grep json | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {cache_urls.aug_dataset}/train_data | grep json | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_files\n",
    "from utils.coco import load as load_coco\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def build_pandas_df(_dir):\n",
    "    pack = []\n",
    "    files = get_files(_dir, \"*.json\")\n",
    "    for file in tqdm(files):\n",
    "        coco_data = load_coco(file)\n",
    "        \n",
    "        row = {\n",
    "            \"file_base_name\"       : None,\n",
    "            \"image_file_bytes\"     : None,\n",
    "            \"coco_json_file_bytes\" : None,\n",
    "            \"dataset_folder_meta\"  : os.path.basename(os.path.dirname(file)),\n",
    "        }\n",
    "        \n",
    "        if len(coco_data['images']) != 1:\n",
    "            continue\n",
    "        \n",
    "        with open(coco_data['images'][0]['file_name'], 'rb') as img_fd:\n",
    "            row['image_file_bytes'] = img_fd.read()\n",
    "        \n",
    "        row[\"file_base_name\"] = osp.basename(coco_data['images'][0]['file_name'])\n",
    "        coco_data['images'][0]['file_name'] = row[\"file_base_name\"]\n",
    "        \n",
    "        row['coco_json_file_bytes'] = json.dumps(coco_data).encode('utf-8')\n",
    "        \n",
    "        pack.append(row)\n",
    "    \n",
    "    return pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертируем аугментированные данные в формат `parquet` и загружаем их на *HDFS* используя *Spark*.\n",
    "\n",
    "Поскольку мы создаем и загружаем их с помощью библиотеки `dsml_s8e.spark`, добавляя опцию `option(\"compression\", \"none\")`, мы можем сразу загружать на *HDFS* \"правильные\" `parquet` файлы, которые уже не нужно после этого \"нормализовывать\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data_df = build_pandas_df(os.path.join(cache_urls.aug_dataset, \"eval_data\"))\n",
    "eval_data_df = spark.createDataFrame(eval_data_df)\n",
    "eval_data_df.printSchema()\n",
    "print(f\"{a7s_urls.eval_coco_data=}\")\n",
    "eval_data_df.write.mode(\"overwrite\").option(\"compression\", \"none\").parquet(a7s_urls.eval_coco_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_df = build_pandas_df(osp.join(cache_urls.aug_dataset, \"train_data\"))\n",
    "train_data_df = spark.createDataFrame(train_data_df)\n",
    "train_data_df.printSchema()\n",
    "print(f\"{a7s_urls.train_coco_data=}\")\n",
    "train_data_df.write.mode(\"overwrite\").option(\"compression\", \"none\").parquet(a7s_urls.train_coco_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG['Normalize'] = {'type': 'Normalize', 'mean': mean, 'std': std, 'to_rgb': False}\n",
    "CONFIG['CLASSES'] = train_dataset.CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"TRAIN_DATASET_DIRS\" in CONFIG:\n",
    "    del CONFIG[\"TRAIN_DATASET_DIRS\"]\n",
    "if \"EVAL_DATASET_DIRS\" in CONFIG:\n",
    "    del CONFIG[\"EVAL_DATASET_DIRS\"]\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(cache_config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "    \n",
    "!cat {cache_config_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dsml_s8e.store import DSMLStore\n",
    "\n",
    "DSMLStore.copy_file_to_store(os.path.join(cache_urls.cache_data, 'config.json'), os.path.join(a7s_urls.cache_config, 'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#11 #SparkEnvironment.stopSparkSession()\n",
    "\n",
    "SparkEnvironment.stopSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
